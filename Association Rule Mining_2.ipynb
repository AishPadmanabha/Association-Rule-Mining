{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule Mining US Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://raw.githubusercontent.com/cs6220/cs6220.spring2019/master/data/adult/\"\n",
    "names = pd.read_csv(path + \"adult.names\", sep=\"\\n\", header=None)\n",
    "parse_cols = lambda x: x.str.split(\":\", expand=True).iloc[:, 0]\n",
    "columns = np.roll(parse_cols(names.iloc[92:108, 0]), shift=-1)\n",
    "df_adult = pd.read_csv(path + \"adult.data\", sep=\",\", header=None, index_col=False)\n",
    "df_adult.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw dataset is transformed into a format appropriate for association rule mining by dropping all continuous columns and one-hot encoding the remaining columns.  The values for each resulting column should be binary, represented by a 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adult_transformed = df_adult.select_dtypes(exclude=np.number)\n",
    "df_adult_encoded = pd.get_dummies(df_adult_transformed)\n",
    "df_adult_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(df_adult_encoded, min_support=0.1, use_colnames=True, max_len=3)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence for the rule interestingness (metric=\"confidence\") is used to generate rules up to a depth of at least 3 (maxlen=3 or higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_confidence_1 = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold = 0.1).sort_values(by='lift', ascending=False).reset_index()\n",
    "rules_confidence_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the explanations below, I'm basing my answer on the combination of support, confidence and mainly, lift.\n",
    "Also from my understanding, antecedents are present in the dataset and consequents are inferred from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, I can safely say that:\n",
    "- People who are own children are earning less 50K or more than 50K or equal to 50K and not married\n",
    "- The commutative inference of the previous point, people are earning less 50K or more than 50K or equal to 50K and not married are only children\n",
    "- People who are own child are never married and from US\n",
    "- People who are never married and from US are own child\n",
    "- People who are white and not married are own child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lift for the rule interestingness (metric=\"Lift\") is used to generate rules up to a depth of at least 3 (maxlen=3 or higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_lift_1 = association_rules(frequent_itemsets, metric=\"lift\", min_threshold = 0.1).sort_values(by='support', ascending=False).reset_index()\n",
    "rules_lift_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, I can safely say that:\n",
    "1. People who are natively from United States are white people\n",
    "2. The commutative inference of the rule stated above, people who are white natively from the US\n",
    "3. People with income less than 50K or more than or equal to 50k are natively from the US\n",
    "4. People who are from the US with income less than 50K or more than or equal to 50K\n",
    "5. People who are white have income less than 50K or more than or equal to 50K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top rules are compared using the two interestingness measures for the same levels of support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be comparing the same process used for the above produced results first for the min threshold 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_confidence_1 = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold = 0.1)\n",
    "rules_lift_1 = association_rules(frequent_itemsets, metric=\"lift\", min_threshold = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_confidence_2 = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold = 0.8)\n",
    "rules_lift_2 = association_rules(frequent_itemsets, metric=\"lift\", min_threshold = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_confidence_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_confidence_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_lift_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_lift_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon comparing the above results of confidence and lift for the same value of supports (0.1 and 0.8), I am getting the same results for lift and different results for confidence. The lift values, from my understanding, is one of the most important criteria as higher lift values mean less likeliness of randomness (so more likely to occur). Support seems a little less and alarming for the top rules but confidence seems better, so the rules are likely to occur according to the confidence.\n",
    "\n",
    "Although everything is justifyable, I'm not able to pin point which one metric makes most sense.\n",
    "Because in the case of confidence_2, a Private working class person can be white, and according to confidence_1, a Bachelor can be working for the private sector. Lift as a parameter seems pretty straigt-forward, the top results are less likely to occur at random and thus the result makes sense.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
